# -*- coding: utf-8 -*-
"""Diabetes prediction using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wr1ZD4q57b7whCNX7OLQrP6ZczhGu48H

importing the dependencies
"""

#numpy is used to create numpy arrays and helpful in processing
import numpy as np
#pandas is used for creating dataframes, used to put data in a structured tables
import pandas as pd
#standardscaler is used to standardize the data
from sklearn.preprocessing import StandardScaler
#train_test_split used to split our data into training and test data
from sklearn.model_selection import train_test_split
#import support vector machine
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Data collection and Analysis

PIMA Diabetes Dataset
"""

#loading the diabetes dataset to pandas data frame
diabetes_dataset = pd.read_csv('/content/sample_data/diabetes.csv')

#way to get more info about a function pd.read_csv?
# pd.read_csv? - Read a comma-separated values (csv) file into DataFrame.

#printing the first five rows of the data set
diabetes_dataset.head()

# number of rows and columns in the dataset
#data taken from 769 people, with 9 features, with one being the diagnosis
diabetes_dataset.shape

# getting the statistical measures of the data
diabetes_dataset.describe()
#gives std deviation, mean of the data etc

#50 % of glucose of people is less than 117 etc

diabetes_dataset['Outcome'].value_counts()

# 1 diabetic and 0 non diabetic

"""0 non
1 diab
"""

diabetes_dataset.groupby('Outcome').mean()
#group dataset based on data labels

#separating dta and labels
#drppong the column outcome, axis =1 to indicate dropping a column and axis=0 for dropping a row
X = diabetes_dataset.drop(columns='Outcome', axis=1)
Y = diabetes_dataset['Outcome']

print(X)

print(Y)

"""DATA STANDARDISATION
we will try to standardize the dtata in a particular range, as you can see each attributes ahave varying range

"""

scaler = StandardScaler()

scaler.fit(X)

Standardized_data = scaler.transform(X)

print(Standardized_data)

#all values in almost the similar range

X = Standardized_data
Y = diabetes_dataset['Outcome']
#we'll use this data to train our model.
#basically x represents the data and Y represents the model

print(X)
print(Y)

"""Train Test split"""

#x data is splitted into two arrays xtrain and x test.
#we'll train our ml model with the xtrain data, once trained we'll evaluate our model with the test data
#y tarin reprsesnts all the labels for this x train data, similarly for y test and x test
#train_test_split gives four output
# parameters x, y = old datasets,; 0.2 reprsesnts 20% data, how much data for test data rest as test data, stratify to equally split diabetic and non diabetic data among the train and test data
# random state = 2 , index or serial number for splitting the data
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, stratify=Y,)

print(X.shape, X_train.shape, X_test.shape)

"""Training the model"""

classifier = svm.SVC(kernel='linear')
#linear model

#training the svm classiifer
classifier.fit(X_train,Y_train)

"""Model evaluation

Accuracy score
"""

# accuracy score on the training data
#storing the prediction of x_train in the xtrainpred and comapring it with ytrain in the next step
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ',training_data_accuracy )

#accuracy score of test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ',test_data_accuracy )

"""Making a predictive system"""

#non diabetic values
#input_data = (4,110,92,9,9,37.6,0.191,30)

#diabetic values
input_data=(5,166,72,19,175,25.8,0.587,51)

#changing the input data to a numoy array
input_data_as_numpy_array = np.asarray(input_data)

#w reshape the array as we are predicting for one instance
#our model is tarind om 768 examples and 8 columns, but in this case we are just usingone data point , what the model expects is 768 data values, cause confusion. Reshape tells the model we need prediction for only one dat point
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#we cannot we have the values as such, as we have standardized the data, so we need to std pur input datastandardize the input data
std_data = scaler.transform(input_data_reshaped)

print(std_data)

#we have created and stored the trained svm model in the variable 'classifier'
#predict fn to predict the label
prediction = classifier.predict(std_data)
print("\n",prediction)
if (prediction[0] ==0):
  #prediction is list and it gives only a element. o to access the first value in the list.
  #o/p will be a list and not an integer. it prints a zero or one inside the list
  print("\n\nperson is non-diabetic\n\n")
else:
  print("\n\nperson is diabetic\n\n")

